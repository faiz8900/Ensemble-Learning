{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Ensemble Learning**"
      ],
      "metadata": {
        "id": "0HNDHkaifa-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it."
      ],
      "metadata": {
        "id": "32X5Zom8fa7U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Ensemble learning combines multiple machine learning models, or \"learners,\" to produce a single, more robust prediction than any individual model could achieve alone. The key idea is the \"wisdom of crowds\": by strategically combining the outputs of diverse models, the ensemble can mitigate errors, reduce overfitting, and improve generalization by canceling out individual model weaknesses and reinforcing collective strengths."
      ],
      "metadata": {
        "id": "EeWusflVfnho"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the difference between Bagging and Boosting?"
      ],
      "metadata": {
        "id": "YlBY0rlPfzvZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Bagging:\n",
        "Parallel Training: Bagging trains multiple base models (e.g., decision trees) in parallel.\n",
        "Data Sampling: Each base model is trained on a different subset of the original training data, generated by bootstrap sampling (random sampling with replacement). This introduces diversity among the models.\n",
        "Independent Models: The base models are built independently of each other.\n",
        "Variance Reduction: Bagging primarily aims to reduce variance, making the overall model more stable and less prone to overfitting."
      ],
      "metadata": {
        "id": "32P_2JO3f3Ob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Boosting:\n",
        "Sequential Training:\n",
        "Boosting trains multiple base models sequentially, with each subsequent model learning from the errors of its predecessors.\n",
        "Weighted Data:\n",
        "Boosting assigns weights to the training data points. Data points that were misclassified by previous models receive higher weights, forcing subsequent models to focus on these \"difficult\" examples.\n",
        "Dependent Models:\n",
        "The construction of new models is influenced by the performance of previously built models."
      ],
      "metadata": {
        "id": "Lq05qX3ggF8i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is bootstrap sampling and what role does it play in Bagging methods\n",
        "like Random Forest?\n"
      ],
      "metadata": {
        "id": "hH5sQo8QgKDR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Bootstrap sampling is a resampling technique where multiple subsets of a dataset are created by randomly selecting data points with replacement. This means that a single data point can be selected multiple times within the same subset, and some data points from the original dataset may not be included in a given subset. Each bootstrap sample has the same size as the original dataset.\n",
        "In Bagging methods, such as Random Forest, bootstrap sampling plays a crucial role in creating diverse training sets for the individual base learners (e.g., decision trees).\n",
        "Role in Bagging Methods like Random Forest:\n",
        "Creating Diverse Training Sets:\n",
        "Bootstrap sampling generates multiple distinct training sets from the original dataset. Each decision tree in a Random Forest is trained on a different bootstrap sample. This ensures that each tree is exposed to a slightly different view of the data, promoting diversity among the individual models."
      ],
      "metadata": {
        "id": "4hWUcNXlgQX-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?"
      ],
      "metadata": {
        "id": "ELoYnVINhCtN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- OOB (out-of-bag) score is a performance metric for a machine learning model, specifically for ensemble models such as random forests. It is calculated using the samples that are not used in the training of the model, which is called out-of-bag samples."
      ],
      "metadata": {
        "id": "8jUKIS69hSja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest.\n"
      ],
      "metadata": {
        "id": "ZBq7x20UhTsa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Single Decision Tree:\n",
        "Impurity-based Importance (Gini Importance or Mean Decrease in Impurity - MDI):\n",
        "A single Decision Tree typically calculates feature importance based on how much each feature reduces impurity (e.g., Gini impurity for classification, mean squared error for regression) when used for splitting nodes. Features that lead to larger reductions in impurity are considered more important.\n",
        "Potential for Instability and Bias:\n",
        "The importance scores in a single tree can be sensitive to small changes in the data, leading to instability. They can also be biased towards features with many unique values (high cardinality features) as these offer more potential split points. Highly correlated features might lead to one being assigned high importance while the other is overlooked, even if both are equally predictive."
      ],
      "metadata": {
        "id": "RCtqyiLnhXs6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Random Forest:\n",
        "Ensemble Averaging of Impurity-based Importance:\n",
        "Random Forests calculate feature importance by averaging the impurity-based importance scores across all the individual Decision Trees within the forest. This averaging process helps to reduce the instability and variance observed in single trees.\n",
        "Permutation Importance:\n",
        "An alternative and often more robust method in Random Forests is Permutation Importance. This involves shuffling the values of a single feature in the out-of-bag (OOB) samples (or a separate validation set) and measuring the resulting decrease in model performance (e.g., accuracy, F1-score). A larger decrease indicates higher importance for that feature. Permutation importance is less prone to the bias towards high-cardinality features seen in impurity-based methods."
      ],
      "metadata": {
        "id": "EX4kwpglhj42"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python program to:\n",
        "● Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "● Train a Random Forest Classifier\n",
        "● Print the top 5 most important features based on feature importance scores."
      ],
      "metadata": {
        "id": "gVcD0XEVhnuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "feature_names = breast_cancer.feature_names\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X, y)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = rf_classifier.feature_importances_\n",
        "\n",
        "# Create a pandas Series for easier sorting and selection\n",
        "feature_importance_series = pd.Series(feature_importances, index=feature_names)\n",
        "\n",
        "# Get the top 5 most important features\n",
        "top_5_features = feature_importance_series.nlargest(5)\n",
        "\n",
        "# Print the top 5 most important features\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(top_5_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zE1MI1yGhyDv",
        "outputId": "f798f2ef-a5e8-4f9f-e87c-e49bea9e6138"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "worst area              0.139357\n",
            "worst concave points    0.132225\n",
            "mean concave points     0.107046\n",
            "worst radius            0.082848\n",
            "worst perimeter         0.080850\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. : Write a Python program to:\n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "● Evaluate its accuracy and compare with a single Decision Tree"
      ],
      "metadata": {
        "id": "gELb9G3Lfa38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Train a single Decision Tree Classifier\n",
        "single_tree = DecisionTreeClassifier(random_state=42)\n",
        "single_tree.fit(X_train, y_train)\n",
        "single_tree_predictions = single_tree.predict(X_test)\n",
        "single_tree_accuracy = accuracy_score(y_test, single_tree_predictions)\n",
        "\n",
        "# Train a Bagging Classifier using Decision Trees\n",
        "bagging_classifier = BaggingClassifier(estimator=DecisionTreeClassifier(random_state=42),\n",
        "                                       n_estimators=10,\n",
        "                                       random_state=42)\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "bagging_predictions = bagging_classifier.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_predictions)\n",
        "\n",
        "# Print the accuracies\n",
        "print(f\"Accuracy of a single Decision Tree: {single_tree_accuracy:.4f}\")\n",
        "print(f\"Accuracy of the Bagging Classifier: {bagging_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wio5E1MiJ_M",
        "outputId": "b6d43de4-f3e6-4cd0-9d5f-33dbe8dd943e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of a single Decision Tree: 1.0000\n",
            "Accuracy of the Bagging Classifier: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to:\n",
        "● Train a Random Forest Classifier\n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "● Print the best parameters and final accuracy\n"
      ],
      "metadata": {
        "id": "Hlbmqq9ZiVku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'n_estimators': [50, 100, 200]\n",
        "}\n",
        "\n",
        "# Create a Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Create GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters\n",
        "print(\"Best parameters found by GridSearchCV:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# Evaluate the model with the best parameters on the test set\n",
        "best_rf_classifier = grid_search.best_estimator_\n",
        "best_predictions = best_rf_classifier.predict(X_test)\n",
        "final_accuracy = accuracy_score(y_test, best_predictions)\n",
        "\n",
        "# Print the final accuracy\n",
        "print(f\"Accuracy with best parameters: {final_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdAi-dYOipic",
        "outputId": "b748f48d-a623-434d-c71c-f9b8db8bcd9a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found by GridSearchCV:\n",
            "{'max_depth': None, 'n_estimators': 50}\n",
            "Accuracy with best parameters: 0.9720\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program to:\n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "● Compare their Mean Squared Errors (MSE)"
      ],
      "metadata": {
        "id": "L5BBqpRbiwYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the dataset\n",
        "california_housing = fetch_california_housing()\n",
        "X = california_housing.data\n",
        "y = california_housing.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Train a Bagging Regressor using Decision Trees\n",
        "bagging_regressor = BaggingRegressor(estimator=DecisionTreeRegressor(random_state=42),\n",
        "                                     n_estimators=10,\n",
        "                                     random_state=42)\n",
        "bagging_regressor.fit(X_train, y_train)\n",
        "bagging_predictions = bagging_regressor.predict(X_test)\n",
        "bagging_mse = mean_squared_error(y_test, bagging_predictions)\n",
        "\n",
        "# Train a Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_regressor.fit(X_train, y_train)\n",
        "rf_predictions = rf_regressor.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_predictions)\n",
        "\n",
        "# Print the Mean Squared Errors\n",
        "print(f\"Mean Squared Error of Bagging Regressor: {bagging_mse:.4f}\")\n",
        "print(f\"Mean Squared Error of Random Forest Regressor: {rf_mse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2dwsOHNi1nP",
        "outputId": "a597ec45-b724-41c1-9afe-640850191795"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error of Bagging Regressor: 0.2787\n",
            "Mean Squared Error of Random Forest Regressor: 0.2542\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "● Choose between Bagging or Boosting\n",
        "● Handle overfitting\n",
        "● Select base models\n",
        "● Evaluate performance using cross-validation\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n"
      ],
      "metadata": {
        "id": "5gge-STIjD8C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 1. Choose between Bagging and Boosting:\n",
        "\n",
        "Consider the problem characteristics: Loan default prediction is typically a binary classification problem where misclassifying a defaulting loan as non-defaulting (false negative) can be more costly than misclassifying a non-defaulting loan as defaulting (false positive).\n",
        "Bagging (like Random Forest): Generally good at reducing variance and preventing overfitting. It trains models in parallel, making it less sensitive to noisy data and outliers. This can be beneficial in financial data which might contain errors or unusual transactions.\n",
        "Boosting (like Gradient Boosting or XGBoost): Generally good at reducing bias and can achieve higher accuracy by sequentially focusing on misclassified instances. However, it can be more prone to overfitting if not carefully tuned.\n",
        "Decision: For loan default prediction, where robustness and handling noisy data are important, Bagging (specifically Random Forest) is often a good starting point. If initial models show high bias, Boosting can be explored, but with careful attention to regularization to avoid overfitting. You could also consider Stacking to combine the strengths of different models."
      ],
      "metadata": {
        "id": "dpQT6UEZjLFh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 2. Handle Overfitting:\n",
        "\n",
        "Data Splitting: Split the data into training, validation (optional but recommended for hyperparameter tuning), and testing sets.\n",
        "Cross-Validation: Use k-fold cross-validation during training to get a more robust estimate of model performance and to detect overfitting.\n",
        "Bagging/Random Forest Specifics:\n",
        "Limit the maximum depth of individual trees (max_depth).\n",
        "Set a minimum number of samples required to split a node (min_samples_split) and a minimum number of samples required in a leaf node (min_samples_leaf).\n",
        "Consider max_features to limit the number of features considered at each split.\n",
        "Boosting Specifics:\n",
        "Use regularization techniques like L1 and L2 regularization.\n",
        "Control the learning rate (learning_rate).\n",
        "Limit the number of boosting rounds (n_estimators).\n",
        "Use early stopping with a validation set.\n",
        "Ensemble Size: Avoid using an excessively large number of base models, as this can sometimes lead to overfitting (though less common in Bagging)."
      ],
      "metadata": {
        "id": "QDetsmgRjlX8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 3. Select Base Models:\n",
        "\n",
        "Decision Trees: These are commonly used as base models in both Bagging and Boosting due to their simplicity and interpretability.\n",
        "Other Models: Depending on the dataset and complexity, other models can be used as base learners, such as:\n",
        "Linear models (e.g., Logistic Regression)\n",
        "Support Vector Machines (SVMs)\n",
        "Neural Networks (in more complex stacking scenarios)\n",
        "Diversity: For Bagging, using diverse base models (though less common with standard Decision Trees) can improve performance. For Boosting, using weak learners that are slightly better than random chance is sufficient."
      ],
      "metadata": {
        "id": "MO8HOaBSjwFb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 4. Evaluate Performance Using Cross-Validation:\n",
        "\n",
        "Metrics: Choose appropriate evaluation metrics for a classification problem with potential class imbalance (loan default is usually a rare event):\n",
        "Accuracy: Basic measure, but can be misleading with imbalanced data.\n",
        "Precision: Of all predicted defaults, what proportion were actual defaults? (Important to minimize false positives).\n",
        "Recall (Sensitivity): Of all actual defaults, what proportion were correctly identified? (Important to minimize false negatives).\n",
        "F1-Score: Harmonic mean of precision and recall, balancing both.\n",
        "AUC-ROC: Measures the ability of the model to distinguish between the two classes.\n",
        "Confusion Matrix: Provides a detailed breakdown of true positives, true negatives, false positives, and false negatives.\n",
        "Cross-Validation Procedure:\n",
        "Split the training data into k folds.\n",
        "Train the ensemble model k times, each time using k-1 folds for training and the remaining fold for validation.\n",
        "Calculate the chosen evaluation metrics for each fold.\n",
        "Average the metrics across all folds to get a more reliable estimate of the model's performance."
      ],
      "metadata": {
        "id": "luRGQr8lj3uP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 5. Justify how Ensemble Learning Improves Decision-Making in this Real-World Context:\n",
        "\n",
        "Improved Accuracy and Robustness: By combining multiple models, the ensemble can capture more complex patterns in the data and is less likely to be affected by noise or outliers compared to a single model. This leads to more accurate predictions of loan default.\n",
        "Reduced Risk: More accurate predictions help in better risk assessment. The financial institution can make more informed decisions on whether to approve a loan, what interest rate to offer, and what credit limits to set. This reduces the risk of financial losses due to defaults.\n",
        "Better Identification of High-Risk Customers: Ensemble models, especially those that provide feature importance (like Random Forest), can help identify the key factors contributing to loan default. This understanding allows the institution to develop targeted strategies for risk mitigation and customer support.\n",
        "Enhanced Stability: Ensemble models are generally more stable than single models, meaning their performance is less likely to fluctuate significantly with new data. This consistency is crucial in a dynamic financial environment.\n",
        "Handling Non-Linear Relationships: Ensemble methods like Random Forest and Gradient Boosting can effectively capture non-linear relationships between features and the target variable, which are common in complex financial data.\n",
        "Increased Confidence in Predictions: The consensus among multiple models in an ensemble often provides a higher degree of confidence in the predictions, which is important for critical decisions like loan approval."
      ],
      "metadata": {
        "id": "s1O4H_Htj_zp"
      }
    }
  ]
}